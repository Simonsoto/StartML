{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63450ca6",
   "metadata": {},
   "source": [
    "# More topics on Melbourne House Prices\n",
    "\n",
    "This notebeook is a continuation of the notebook `house-pricing`. The idea is to go deep in new topics in ML.\n",
    "\n",
    "\n",
    "## MISSING VALUES\n",
    "\n",
    "Missing values are the most common data issue that you will be able to find in almost every data set. The best thing is to be prepared to face this challenge. \n",
    "\n",
    "Most of the machine learning libraries (like sci-kit learn) cannot deal with missing values. These raise an error if a model is built using data with missing values. To avoid these, there are some strategies.\n",
    " \n",
    "**1. Drop columns with missing values**\n",
    "\n",
    "This is the mos simple option but not the most recommended as it has to be clear that the column that is removed does not add anything to the model.\n",
    "\n",
    "**2. Filling the missing values (Imputation)**\n",
    "\n",
    "**Filling** the missing values with some other value. An example of a filling value is the _mean_ but in case you have [skewed](https://en.wikipedia.org/wiki/Skewness) data you might want to use the median, or even the mode. \n",
    "\n",
    "This is probably the best method that can be used. This depends on that the value that is added really makes sense as a value for the numerical values that are in the range of the column. \n",
    "\n",
    "**3. Extension of filling**\n",
    "\n",
    "Filling is the standard approach, and it usually works well. There might be disadvantages as the value that is used to fill in the missing might not be completely accurate, or the rows with the missig values can be unique in some other way. In that case, the model would make better prediction considering which values were originlly missing. For that purpose, for each column with a missing value, a new column is added `\"original_name\"_was_missing` that has as values `True`if the value in the `original_name` was missing and `False` otherwise. This trick can help in some cases but is not always the case.\n",
    "\n",
    "Now, let us test those three in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee17188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             1460 non-null   int64  \n",
      " 1   MSSubClass     1460 non-null   int64  \n",
      " 2   MSZoning       1460 non-null   object \n",
      " 3   LotFrontage    1201 non-null   float64\n",
      " 4   LotArea        1460 non-null   int64  \n",
      " 5   Street         1460 non-null   object \n",
      " 6   Alley          91 non-null     object \n",
      " 7   LotShape       1460 non-null   object \n",
      " 8   LandContour    1460 non-null   object \n",
      " 9   Utilities      1460 non-null   object \n",
      " 10  LotConfig      1460 non-null   object \n",
      " 11  LandSlope      1460 non-null   object \n",
      " 12  Neighborhood   1460 non-null   object \n",
      " 13  Condition1     1460 non-null   object \n",
      " 14  Condition2     1460 non-null   object \n",
      " 15  BldgType       1460 non-null   object \n",
      " 16  HouseStyle     1460 non-null   object \n",
      " 17  OverallQual    1460 non-null   int64  \n",
      " 18  OverallCond    1460 non-null   int64  \n",
      " 19  YearBuilt      1460 non-null   int64  \n",
      " 20  YearRemodAdd   1460 non-null   int64  \n",
      " 21  RoofStyle      1460 non-null   object \n",
      " 22  RoofMatl       1460 non-null   object \n",
      " 23  Exterior1st    1460 non-null   object \n",
      " 24  Exterior2nd    1460 non-null   object \n",
      " 25  MasVnrType     1452 non-null   object \n",
      " 26  MasVnrArea     1452 non-null   float64\n",
      " 27  ExterQual      1460 non-null   object \n",
      " 28  ExterCond      1460 non-null   object \n",
      " 29  Foundation     1460 non-null   object \n",
      " 30  BsmtQual       1423 non-null   object \n",
      " 31  BsmtCond       1423 non-null   object \n",
      " 32  BsmtExposure   1422 non-null   object \n",
      " 33  BsmtFinType1   1423 non-null   object \n",
      " 34  BsmtFinSF1     1460 non-null   int64  \n",
      " 35  BsmtFinType2   1422 non-null   object \n",
      " 36  BsmtFinSF2     1460 non-null   int64  \n",
      " 37  BsmtUnfSF      1460 non-null   int64  \n",
      " 38  TotalBsmtSF    1460 non-null   int64  \n",
      " 39  Heating        1460 non-null   object \n",
      " 40  HeatingQC      1460 non-null   object \n",
      " 41  CentralAir     1460 non-null   object \n",
      " 42  Electrical     1459 non-null   object \n",
      " 43  1stFlrSF       1460 non-null   int64  \n",
      " 44  2ndFlrSF       1460 non-null   int64  \n",
      " 45  LowQualFinSF   1460 non-null   int64  \n",
      " 46  GrLivArea      1460 non-null   int64  \n",
      " 47  BsmtFullBath   1460 non-null   int64  \n",
      " 48  BsmtHalfBath   1460 non-null   int64  \n",
      " 49  FullBath       1460 non-null   int64  \n",
      " 50  HalfBath       1460 non-null   int64  \n",
      " 51  BedroomAbvGr   1460 non-null   int64  \n",
      " 52  KitchenAbvGr   1460 non-null   int64  \n",
      " 53  KitchenQual    1460 non-null   object \n",
      " 54  TotRmsAbvGrd   1460 non-null   int64  \n",
      " 55  Functional     1460 non-null   object \n",
      " 56  Fireplaces     1460 non-null   int64  \n",
      " 57  FireplaceQu    770 non-null    object \n",
      " 58  GarageType     1379 non-null   object \n",
      " 59  GarageYrBlt    1379 non-null   float64\n",
      " 60  GarageFinish   1379 non-null   object \n",
      " 61  GarageCars     1460 non-null   int64  \n",
      " 62  GarageArea     1460 non-null   int64  \n",
      " 63  GarageQual     1379 non-null   object \n",
      " 64  GarageCond     1379 non-null   object \n",
      " 65  PavedDrive     1460 non-null   object \n",
      " 66  WoodDeckSF     1460 non-null   int64  \n",
      " 67  OpenPorchSF    1460 non-null   int64  \n",
      " 68  EnclosedPorch  1460 non-null   int64  \n",
      " 69  3SsnPorch      1460 non-null   int64  \n",
      " 70  ScreenPorch    1460 non-null   int64  \n",
      " 71  PoolArea       1460 non-null   int64  \n",
      " 72  PoolQC         7 non-null      object \n",
      " 73  Fence          281 non-null    object \n",
      " 74  MiscFeature    54 non-null     object \n",
      " 75  MiscVal        1460 non-null   int64  \n",
      " 76  MoSold         1460 non-null   int64  \n",
      " 77  YrSold         1460 non-null   int64  \n",
      " 78  SaleType       1460 non-null   object \n",
      " 79  SaleCondition  1460 non-null   object \n",
      " 80  SalePrice      1460 non-null   int64  \n",
      "dtypes: float64(3), int64(35), object(43)\n",
      "memory usage: 924.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# import all the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "df = pd.read_csv('data/train.csv')\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00308b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the main variable\n",
    "y = df.SalePrice\n",
    "y = y.fillna(y.median())\n",
    "\n",
    "X = df.drop(['SalePrice'], axis = 1)\n",
    "\n",
    "#For simplicity select only the numerical variables\n",
    "X = X.select_dtypes(exclude = ['object'])\n",
    "\n",
    "# Divide the dataset\n",
    "X_train, X_valid, y_train, y_valid= train_test_split(X, y, \n",
    "                                                     train_size = 0.8,\n",
    "                                                     test_size = 0.2)\n",
    "\n",
    "#function to compare the different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators =10, random_state = 100)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fcbe71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for case1: Dropping columns with missng values : 17540.179109589044\n"
     ]
    }
   ],
   "source": [
    "# Case 1. drop the columns that contain missing values\n",
    "cols_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "\n",
    "reduced_X_train = X_train.drop(cols_missing, axis =1)\n",
    "reduced_X_valid = X_valid.drop(cols_missing, axis = 1)\n",
    "\n",
    "print('MAE for case1: Dropping columns with missng values : {}'.format(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5c43e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE case 2: Filling/Imputation is:17616.610958904108\n"
     ]
    }
   ],
   "source": [
    "#Case 2: Imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "## Imputation\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "imputed_X_train = pd.DataFrame(imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(imputer.transform(X_valid))\n",
    "\n",
    "## gave back the original names to the columns\n",
    "\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "print('MAE case 2: Filling/Imputation is:{}'.format(score_dataset(imputed_X_train,imputed_X_valid,y_train, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a0caec",
   "metadata": {},
   "source": [
    "There is already a considerable reduction in the MAE in between the first two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c556f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE case 3: Imputer extended: 18006.705479452055\n"
     ]
    }
   ],
   "source": [
    "#Case three, Extension of filling\n",
    "# make copies of the X sets\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "#make new column indicating what will be imputed\n",
    "for col in cols_missing:\n",
    "    X_train_plus[col+'_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col+'_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "#Generate the imputer and change the values\n",
    "imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(imputer.transform(X_valid_plus))\n",
    "\n",
    "# rename the columns of the imputer with the previous names\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns\n",
    "\n",
    "print('MAE case 3: Imputer extended: {}'.format(score_dataset(imputed_X_train_plus,imputed_X_valid_plus,y_train, y_valid)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1335a7a4",
   "metadata": {},
   "source": [
    "As seen in the last result, adding that extra layer of sophistication is not reflected in the overall model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4be2ce",
   "metadata": {},
   "source": [
    "## Categorical Data\n",
    "\n",
    "A categorical variable takes only a limited number of values. Think for example surveys in which you are asked your mood and the possible answers are 'happy','normal', 'sad'. These are categories. \n",
    "\n",
    "How to use the data? \n",
    "\n",
    "**1. Drop Categorical variables:**\n",
    "This is again the easiest approach. Remove those columns that contain categorical values.\n",
    "\n",
    "**2. Ordinal Encoding:**\n",
    "Assign a unique value to each of the categories. In this approach, it is assumed that there is an ordering of the variables. In the case, 'happy'>'normal'>'sad'\n",
    "The assumtption makes sense in this example but this might not be the case all the time. These variables as referred as 'ordinal variables'\n",
    "\n",
    "_Be carefull in this approach as both the training and the validation dataset should have the same collection of categories_\n",
    "\n",
    "**3. One-hot Encoding:**\n",
    "One-hot encoding creates new columns indicating the presence (or absence) of each of the possible values in the original data. Each category is represented by a column that has the following form. For variable X and cateogry i, the variable $X_i$ takes the follwing values for the observation j\n",
    "$$ X_i =\\begin{cases}1 &\\text{if }X(j) = i\\\\ 0& \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "Therefore, at the end, the new vectors contain the information that it was contained in the original categorical variable. \n",
    "\n",
    "In this approach, there is no order assumed, therefore this is a good approach when there is not a clear order in the categories. These are 'nominal variables'. One drawback from this approach is that it does not perform well if the categorical variable has too many categories where too many can be as small as 15 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be97bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble  import RandomForestRegressor\n",
    "\n",
    "## Read the data set as before we were not considering categorical \n",
    "# variables\n",
    "df_2 = pd.read_csv('data/train.csv')\n",
    "# extract the y value\n",
    "y = df_2.SalePrice \n",
    "y = y.fillna(y.median())\n",
    "\n",
    "X = df_2.drop('SalePrice', axis = 1)\n",
    "\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X,y,\n",
    "                                                                train_size=0.8,\n",
    "                                                                test_size= 0.2,\n",
    "                                                                random_state = 1) \n",
    "\n",
    "# For this part of the exersice, the data from the first approach\n",
    "# to missing values is going to be used. \n",
    "\n",
    "cols_missing = [col for col in X_train_full.columns if X_train_full[col].isna().any()]\n",
    "\n",
    "X_train_full = X_train_full.drop(columns = cols_missing, axis = 1)\n",
    "X_valid_full = X_valid_full.drop(columns = cols_missing, axis = 1)\n",
    "\n",
    "# check the variables that are categorical with low cardinality \n",
    "# and also the ones with numerical values\n",
    "\n",
    "# Categorical values:_\n",
    "low_cardinality = [name for name in X_train_full.columns \n",
    "                   if X_train_full[name].dtype == 'object' and X_train_full[name].nunique()<=10]\n",
    "\n",
    "#Numerical values\n",
    "numerical = [name for name in X_train_full.columns if X_train_full[name].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Columns to study\n",
    "my_cols = low_cardinality + numerical\n",
    "\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d87df8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>Condition1</th>\n",
       "      <th>Condition2</th>\n",
       "      <th>BldgType</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Feedr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Duplex</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>2fmCon</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>114</td>\n",
       "      <td>210</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>...</td>\n",
       "      <td>400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>...</td>\n",
       "      <td>575</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>RL</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>...</td>\n",
       "      <td>398</td>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     MSZoning Street LotShape LandContour Utilities LotConfig LandSlope  \\\n",
       "921        RL   Pave      Reg         Lvl    AllPub    Inside       Gtl   \n",
       "520        RL   Pave      Reg         Lvl    AllPub    Inside       Gtl   \n",
       "401        RL   Pave      IR1         Lvl    AllPub    Inside       Gtl   \n",
       "280        RL   Pave      Reg         Lvl    AllPub    Inside       Gtl   \n",
       "1401       RL   Pave      IR1         Lvl    AllPub    Inside       Gtl   \n",
       "\n",
       "     Condition1 Condition2 BldgType  ... GarageArea WoodDeckSF OpenPorchSF  \\\n",
       "921       Feedr       Norm   Duplex  ...          0          0          70   \n",
       "520        Norm       Norm   2fmCon  ...          0        220         114   \n",
       "401        Norm       Norm     1Fam  ...        400          0           0   \n",
       "280        Norm       Norm     1Fam  ...        575          0          84   \n",
       "1401       Norm       Norm     1Fam  ...        398        100          75   \n",
       "\n",
       "     EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold  \n",
       "921              0         0           0        0       0      9   2008  \n",
       "520            210         0           0        0       0      8   2008  \n",
       "401              0         0           0        0       0      7   2006  \n",
       "280              0       196           0        0       0      1   2007  \n",
       "1401             0         0           0        0       0      4   2008  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c07fe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of categorical variables\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd32e5c",
   "metadata": {},
   "source": [
    "To measure the different approaches, the function `score_dataset` defined above will be used again. This is again defined here for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1914ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the function score_dataset\n",
    "\n",
    "def socre_dataset(X_train, X_valid, y_train, y_valid ):\n",
    "    '''\n",
    "    Function to socre the data set based on the approach that was given \n",
    "    '''\n",
    "    \n",
    "    model = RandomForestRegressor(n_estimators=100,\n",
    "                                  random_state=100)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    #MAke the predictions\n",
    "    predictions = (X_valid) \n",
    "    \n",
    "    error = mean_absolute_error(y_valid, predictions)\n",
    "    return round(error,4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe1abc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop categorical variables): 17185.62705479452\n"
     ]
    }
   ],
   "source": [
    "# Case 1: Drop categorical variables\n",
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "print(\"MAE from Approach 1 (Drop categorical variables): {}\".format(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7c1a96",
   "metadata": {},
   "source": [
    "With an `OrdinalEncoder` from `sklearn`, a random integer is assign to each unique value in the categorical variable. This is a common practice that is simpler than giving customized labels. However, it is expected that the performance boosts if the better-informed labels for the ordinal variables are given.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5d4da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_checker(df1, df2, cat_columns = None):\n",
    "    '''\n",
    "    Function to check if the columns in both dataframes have\n",
    "    the same categories in the cat_columns. If not, return which \n",
    "    columns have different categories in both dataframes. This is \n",
    "    important for the Ordinal encoding and the one-hot encoding\n",
    "    as they are trained with one dataframe and the other one will be \n",
    "    fitted based on the previous results. If there is new information \n",
    "    in the second, then it will raise an error.\n",
    "    '''\n",
    "    cols_to_drop = []\n",
    "    for col in cat_columns:\n",
    "        categories_df1 = set(df1[col].unique())\n",
    "        categories_df2 = set(df2[col].unique())\n",
    "        if len(categories_df1 ^ categories_df2)>0:\n",
    "            cols_to_drop = cols_to_drop + [col]\n",
    "    return cols_to_drop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ccaada9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2 (Ordinal Encoding):17312.79075342466\n"
     ]
    }
   ],
   "source": [
    "# Case 2: Ordinal Encoding\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "#Make sure that both data sets have the same categories.\n",
    "not_same_categories = category_checker(X_train,X_valid, object_cols)\n",
    "\n",
    "# Make copy of the training a validation sets\n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# remove the categorical variables result from category_checker()\n",
    "# First from the train and validation data\n",
    "label_X_train = label_X_train.drop(not_same_categories, axis =1)\n",
    "label_X_valid = label_X_valid.drop(not_same_categories, axis =1)\n",
    "\n",
    "#From the columns that are going to be checked\n",
    "object_cols_reduced = list(set(object_cols) - set(not_same_categories))\n",
    "\n",
    "\n",
    "# apply the ordinal encoder to each of the categorical variables\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "label_X_train[object_cols_reduced] = ordinal_encoder.fit_transform(X_train[object_cols_reduced])\n",
    "\n",
    "label_X_valid[object_cols_reduced] = ordinal_encoder.transform(X_valid[object_cols_reduced])\n",
    "\n",
    "\n",
    "print(\"MAE from Approach 2 (Ordinal Encoding):{}\".format(score_dataset(label_X_train, label_X_valid, y_train, y_valid))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13b6054c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 3 (One-hot Encoding)\n",
      "16874.49383561644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simonsoto/opt/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:1675: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/simonsoto/opt/miniconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:1675: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Case 3 One-Hot Encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "## Apply one hot encoder to each columns with categorical data\n",
    "oh_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "oh_cols_train = pd.DataFrame(oh_encoder.fit_transform(X_train[object_cols]))\n",
    "oh_cols_valid = pd.DataFrame(oh_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "## one-hot encoding removed index: put it back\n",
    "oh_cols_train.index = X_train.index\n",
    "oh_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove the categorical columns (those were replaced with the one-hot-encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis = 1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis = 1)\n",
    "\n",
    "# Add the one-hot- encoding columns to the numerical ones\n",
    "oh_X_train = pd.concat([num_X_train, oh_cols_train], axis =1)\n",
    "oh_X_valid = pd.concat([num_X_valid, oh_cols_valid], axis =1)\n",
    "\n",
    "print('MAE from Approach 3 (One-hot Encoding)')\n",
    "print(score_dataset(oh_X_train, oh_X_valid, y_train, y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae294785",
   "metadata": {},
   "source": [
    "### Which is the winning approach? \n",
    "\n",
    "Dropping the categorical variables usually is the one that perform the worst and the best is the one-hot encoding, but it varies in a case-by-case basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43872ebd",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "There are always categorical values all around the world and it is good to know how to handle it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
