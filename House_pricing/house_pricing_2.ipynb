{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63450ca6",
   "metadata": {},
   "source": [
    "# More topics on Melbourne House Prices\n",
    "\n",
    "This notebeook is a continuation of the notebook `house-pricing`. The idea is to go deep in new topics in ML.\n",
    "\n",
    "\n",
    "## MISSING VALUES\n",
    "\n",
    "Missing values are the most common data issue that you will be able to find in almost every data set. The best thing is to be prepared to face this challenge. \n",
    "\n",
    "Most of the machine learning libraries (like sci-kit learn) cannot deal with missing values. These raise an error if a model is built using data with missing values. To avoid these, there are some strategies.\n",
    " \n",
    "**1. Drop columns with missing values**\n",
    "\n",
    "This is the mos simple option but not the most recommended as it has to be clear that the column that is removed does not add anything to the model.\n",
    "\n",
    "**2. Filling the missing values (Imputation)**\n",
    "\n",
    "**Filling** the missing values with some other value. An example of a filling value is the _mean_ but in case you have [skewed](https://en.wikipedia.org/wiki/Skewness) data you might want to use the median, or even the mode. \n",
    "\n",
    "This is probably the best method that can be used. This depends on that the value that is added really makes sense as a value for the numerical values that are in the range of the column. \n",
    "\n",
    "**3. Extension of filling**\n",
    "\n",
    "Filling is the standard approach, and it usually works well. There might be disadvantages as the value that is used to fill in the missing might not be completely accurate, or the rows with the missig values can be unique in some other way. In that case, the model would make better prediction considering which values were originlly missing. For that purpose, for each column with a missing value, a new column is added `\"original_name\"_was_missing` that has as values `True`if the value in the `original_name` was missing and `False` otherwise. This trick can help in some cases but is not always the case.\n",
    "\n",
    "Now, let us test those three in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ee17188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34857 entries, 0 to 34856\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Suburb         34857 non-null  object \n",
      " 1   Address        34857 non-null  object \n",
      " 2   Rooms          34857 non-null  int64  \n",
      " 3   Type           34857 non-null  object \n",
      " 4   Price          27247 non-null  float64\n",
      " 5   Method         34857 non-null  object \n",
      " 6   SellerG        34857 non-null  object \n",
      " 7   Date           34857 non-null  object \n",
      " 8   Distance       34856 non-null  float64\n",
      " 9   Postcode       34856 non-null  float64\n",
      " 10  Bedroom2       26640 non-null  float64\n",
      " 11  Bathroom       26631 non-null  float64\n",
      " 12  Car            26129 non-null  float64\n",
      " 13  Landsize       23047 non-null  float64\n",
      " 14  BuildingArea   13742 non-null  float64\n",
      " 15  YearBuilt      15551 non-null  float64\n",
      " 16  CouncilArea    34854 non-null  object \n",
      " 17  Lattitude      26881 non-null  float64\n",
      " 18  Longtitude     26881 non-null  float64\n",
      " 19  Regionname     34854 non-null  object \n",
      " 20  Propertycount  34854 non-null  float64\n",
      "dtypes: float64(12), int64(1), object(8)\n",
      "memory usage: 5.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# import all the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "df = pd.read_csv('data/train.csv')\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "00308b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the main variable\n",
    "y = df.SalePrice\n",
    "y = y.fillna(y.median())\n",
    "\n",
    "X = df.drop(['SalePrice'], axis = 1)\n",
    "\n",
    "#For simplicity select only the numerical variables\n",
    "X = X.select_dtypes(exclude = ['object'])\n",
    "\n",
    "# Divide the dataset\n",
    "X_train, X_valid, y_train, y_valid= train_test_split(X, y, \n",
    "                                                     train_size = 0.8,\n",
    "                                                     test_size = 0.2)\n",
    "\n",
    "#function to compare the different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators =10, random_state = 100)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8fcbe71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for case1: Dropping columns with missng values : 343301.41088616254\n"
     ]
    }
   ],
   "source": [
    "# Case 1. drop the columns that contain missing values\n",
    "cols_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n",
    "\n",
    "reduced_X_train = X_train.drop(cols_missing, axis =1)\n",
    "reduced_X_valid = X_valid.drop(cols_missing, axis = 1)\n",
    "\n",
    "print('MAE for case1: Dropping columns with missng values : {}'.format(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b5c43e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE case 2: Filling/Imputation is:254327.7195829716\n"
     ]
    }
   ],
   "source": [
    "#Case 2: Imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "## Imputation\n",
    "imputer = SimpleImputer()\n",
    "\n",
    "imputed_X_train = pd.DataFrame(imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(imputer.transform(X_valid))\n",
    "\n",
    "## gave back the original names to the columns\n",
    "\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "print('MAE case 2: Filling/Imputation is:{}'.format(score_dataset(imputed_X_train,imputed_X_valid,y_train, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a0caec",
   "metadata": {},
   "source": [
    "There is already a considerable reduction in the MAE in between the first two approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2c556f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE case 3: Imputer extended: 254276.4703580419\n"
     ]
    }
   ],
   "source": [
    "#Case three, Extension of filling\n",
    "# make copies of the X sets\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "#make new column indicating what will be imputed\n",
    "for col in cols_missing:\n",
    "    X_train_plus[col+'_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col+'_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "#Generate the imputer and change the values\n",
    "imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(imputer.transform(X_valid_plus))\n",
    "\n",
    "# rename the columns of the imputer with the previous names\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns\n",
    "\n",
    "print('MAE case 3: Imputer extended: {}'.format(score_dataset(imputed_X_train_plus,imputed_X_valid_plus,y_train, y_valid)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1335a7a4",
   "metadata": {},
   "source": [
    "As seen in the last result, adding that extra layer of sophistication is not reflected in the overall model performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4be2ce",
   "metadata": {},
   "source": [
    "## Categorical Data\n",
    "\n",
    "A categorical variable takes only a limited number of values. Think for example surveys in which you are asked your mood and the possible answers are 'happy','normal', 'sad'. These are categories. \n",
    "\n",
    "How to use the data? \n",
    "\n",
    "**1. Drop Categorical variables:**\n",
    "This is again the easiest approach. Remove those columns that contain categorical values.\n",
    "\n",
    "**2. Ordinal Encoding:**\n",
    "Assign a unique value to each of the categories. In this approach, it is assumed that there is an ordering of the variables. In the case, 'happy'>'normal'>'sad'\n",
    "The assumtption makes sense in this example but this might not be the case all the time. These variables as referred as 'ordinal variables'\n",
    "\n",
    "_Be carefull in this approach as both the training and the validation dataset should have the same collection of categories_\n",
    "\n",
    "**3. One-hot Encoding:**\n",
    "One-hot encoding creates new columns indicating the presence (or absence) of each of the possible values in the original data. Each category is represented by a column that has the following form. For variable X and cateogry i, the variable $X_i$ takes the follwing values for the observation j\n",
    "$$ X_i =\\begin{cases}1 &\\text{if }X(j) = i\\\\ 0& \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "Therefore, at the end, the new vectors contain the information that it was contained in the original categorical variable. \n",
    "\n",
    "In this approach, there is no order assumed, therefore this is a good approach when there is not a clear order in the categories. These are 'nominal variables'. One drawback from this approach is that it does not perform well if the categorical variable has too many categories where too many can be as small as 15 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "be97bb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble  import RandomForestRegressor\n",
    "\n",
    "## Read the data set as before we were not considering categorical \n",
    "# variables\n",
    "df_2 = pd.read_csv('data/train.csv')\n",
    "# extract the y value\n",
    "y = df_2.SalePrice \n",
    "y = y.fillna(y.median())\n",
    "\n",
    "X = df_2.drop('SalePrice', axis = 1)\n",
    "\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(X,y,\n",
    "                                                                train_size=0.8,\n",
    "                                                                test_size= 0.2,\n",
    "                                                                random_state = 1) \n",
    "\n",
    "# For this part of the exersice, the data from the first approach\n",
    "# to missing values is going to be used. \n",
    "\n",
    "cols_missing = [col for col in X_train_full.columns if X_train_full[col].isna().any()]\n",
    "\n",
    "X_train_full = X_train_full.drop(columns = cols_missing, axis = 1)\n",
    "X_valid_full = X_valid_full.drop(columns = cols_missing, axis = 1)\n",
    "\n",
    "# check the variables that are categorical with low cardinality \n",
    "# and also the ones with numerical values\n",
    "\n",
    "# Categorical values:_\n",
    "low_cardinality = [name for name in X_train_full.columns \n",
    "                   if X_train_full[name].dtype == 'object' and X_train_full[name].nunique()<=10]\n",
    "\n",
    "#Numerical values\n",
    "numerical = [name for name in X_train_full.columns if X_train_full[name].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Columns to study\n",
    "my_cols = low_cardinality + numerical\n",
    "\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d87df8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Method</th>\n",
       "      <th>Rooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23491</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14381</th>\n",
       "      <td>h</td>\n",
       "      <td>S</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>h</td>\n",
       "      <td>PI</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16775</th>\n",
       "      <td>h</td>\n",
       "      <td>PI</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Type Method  Rooms\n",
       "23491    h      S      3\n",
       "5998     h      S      4\n",
       "14381    h      S      4\n",
       "1202     h     PI      3\n",
       "16775    h     PI      4"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a707fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of categorical variables\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e414d1c",
   "metadata": {},
   "source": [
    "To measure the different approaches, the function `score_dataset` defined above will be used again. This is again defined here for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "80be5d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the function score_dataset\n",
    "\n",
    "def socre_dataset(X_train, X_valid, y_train, y_valid ):\n",
    "    '''\n",
    "    Function to socre the data set based on the approach that was given \n",
    "    '''\n",
    "    \n",
    "    model = RandomForestRegressor(n_estimators=100,\n",
    "                                  random_state=100)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    #MAke the predictions\n",
    "    predictions = (X_valid)\n",
    "    \n",
    "    error = mean_absolute_error(y_valid, predictions)\n",
    "    return round(error,4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4fea2b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 1 (Drop categorical variables): 348410.8782100967\n"
     ]
    }
   ],
   "source": [
    "# Case 1: Drop categorical variables\n",
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "print(\"MAE from Approach 1 (Drop categorical variables): {}\".format(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288c958e",
   "metadata": {},
   "source": [
    "With an `OrdinalEncoder` from `sklearn`, a random integer is assign to each unique value in the categorical variable. This is a common practice that is simpler than giving customized labels. However, it is expected that the performance boosts if the better-informed labels for the ordinal variables are given.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f1d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_checker(df1, df2, cat_columns = None):\n",
    "    '''\n",
    "    Function to check if the columns in both dataframes have\n",
    "    the same categories in the cat_columns. If not, return which \n",
    "    columns have different classifications.\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "130a923f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE from Approach 2 (Ordinal Encoding):321528.2349797436\n"
     ]
    }
   ],
   "source": [
    "# Case 2: Ordinal Encoding\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "#Make sure that both data sets have the same categories.\n",
    "\n",
    "\n",
    "# Make copy of the training a validation sets\n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Delete the values categories that are not in the training set.\n",
    "for cat_col in object_cols:\n",
    "    categories = label_X_train\n",
    "\n",
    "# apply the ordinal encoder to each of the categorical variables\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "label_X_train[object_cols] = ordinal_encoder.fit_transform(X_train[object_cols])\n",
    "\n",
    "label_X_valid[object_cols] = ordinal_encoder.transform(X_valid[object_cols])\n",
    "\n",
    "\n",
    "print(\"MAE from Approach 2 (Ordinal Encoding):{}\".format(score_dataset(label_X_train, label_X_valid, y_train, y_valid))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cef77aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['CompShg', 'WdShngl', 'WdShake', 'Tar&Grv', 'Metal'], dtype=object),\n",
       " array(['CompShg', 'WdShngl', 'Tar&Grv', 'ClyTile', 'WdShake', 'Membran',\n",
       "        'Roll'], dtype=object))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Case 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b6888c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_X_valid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
